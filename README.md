# ğŸ† LMSYS Chatbot Arena - 3rd Place Solution

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.3.0-orange.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/Transformers-4.43.1-green.svg)](https://huggingface.co/docs/transformers)

> ğŸ¯ **Winning solution** (3rd place) for the [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) competition. This repository implements a comprehensive two-stage training pipeline using multiple model architectures and training techniques for preference learning and response ranking.

![Architecture Overview](docs/images/architecture.png)
*Figure 1: High-level architecture of the solution*

## ğŸ“‹ Table of Contents

- [ğŸ† Competition Overview](#-competition-overview)
- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Setup](#-setup)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”„ Training Pipeline](#-training-pipeline)
- [ğŸ’» Usage](#-usage)
- [âš™ï¸ Model Configurations](#ï¸-model-configurations)
- [ğŸ”§ Key Components](#-key-components)
- [ğŸ“Š Training Techniques](#-training-techniques)
- [ğŸ“ Notes](#-notes)
- [ğŸ“„ License](#-license)
- [ğŸ™ Acknowledgments](#-acknowledgments)

## ğŸ† Competition Overview

The [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) competition focuses on training models to predict which of two chatbot responses is preferred by human evaluators. The solution uses a combination of cross-encoder models, LLM-based reward models, and pseudo-labeling techniques to achieve high performance.

**Competition Links:**
- ğŸŒ [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena)
- ğŸ“Š [Competition Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- ğŸ“š [LMSYS Research](https://lmsys.org/)

## âœ¨ Features

- ğŸ”€ **Multi-Model Ensemble**: Combines cross-encoders, LLM-based reward models, and bi-encoders
- ğŸ”„ **Two-Stage Training**: Stage 1 generates pseudo labels, Stage 2 trains on pseudo-labeled data
- ğŸ·ï¸ **Pseudo-Labeling**: Leverages unlabeled data through iterative pseudo-labeling
- ğŸ” **Test-Time Augmentation (TTA)**: Improves robustness through response swapping
- ğŸ“ **Multiple Training Methods**: Supports SFT, DPO, QLoRA, and reward model training
- âš¡ **Efficient Training**: Uses QLoRA, gradient checkpointing, and mixed precision training
- ğŸ“ˆ **Out-of-Fold Optimization**: Ensemble optimization for better predictions
- ğŸ¯ **High Performance**: Achieved 3rd place in the competition

## ğŸ—ï¸ Architecture

![Training Pipeline](docs/images/training_pipeline.png)
*Figure 2: Two-stage training pipeline*

The solution employs multiple model architectures:

1. **ğŸ”— Cross-Encoder Models**: [DeBERTa](https://huggingface.co/microsoft/deberta-v3-large)-based models that encode prompt-response pairs together
2. **ğŸ¤– LLM-Based Reward Models**: Large language models ([Gemma](https://huggingface.co/google/gemma-2-27b-it), [Llama](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2), [Qwen](https://huggingface.co/Qwen/Qwen2-7B-Instruct)) fine-tuned with QLoRA for preference prediction
3. **ğŸ”€ Bi-Encoder Models**: Siamese networks for separate encoding of responses
4. **ğŸ“Š Sequence Models**: Transformer-based sequence classification models

### Supported Base Models

| Model Family | Variants | Hugging Face Links |
|-------------|----------|-------------------|
| **ğŸŸ¢ Gemma** | Gemma-2 (2B, 27B) | [google/gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it) |
| **ğŸ¦™ Llama** | Llama-3 (8B, 70B) | [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |
| **ğŸŒªï¸ Mistral** | Mistral, Mistral-Nemo | [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) |
| **ğŸ”· Qwen** | Qwen2 variants | [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct) |
| **ğŸ”µ Others** | InternLM, Starling, Zephyr, Mathstral | [InternLM](https://huggingface.co/internlm), [Starling](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha) |

## ğŸš€ Setup

### ğŸ“‹ System Requirements

- **ğŸ–¥ï¸ OS**: Ubuntu 22.04
- **ğŸ® GPUs**: 
  - Training: 2x4090s, 4-8x A100s/4090s/H100s ([vast.ai](https://vast.ai)), or 8xH100s ([Lambda](https://lambdalabs.com))
  - Minimum: 2x GPUs with 24GB+ VRAM for Stage 1
  - Recommended: 8x GPUs with 40GB+ VRAM for Stage 2
- **ğŸ Python**: 3.10 (via conda)
- **ğŸ“¦ Dependencies**: See [environment.yml](environment.yml)

### ğŸ”§ Installation

1. **ğŸ“¥ Clone the repository**:
   ```bash
   git clone <repository-url>
   cd ChatBot-Solution
   ```

2. **ğŸŒ Create conda environment**:
   ```bash
   conda env create -f environment.yml
   conda activate unsloth
   ```

3. **ğŸ” Authenticate with Hugging Face** (required for downloading datasets):
   ```bash
   huggingface-cli login
   ```
   > ğŸ’¡ Get your token from [Hugging Face Settings](https://huggingface.co/settings/tokens)

4. **ğŸ“ Prepare data directories**:
   ```bash
   mkdir -p data/preds data/pseudo
   ```

5. **ğŸ“Š Download competition data**:
   - Place `train.csv` and `test.csv` in the `data/` directory
   - Data format: CSV files with columns `prompt`, `response_a`, `response_b`, `winner_model_a`, `winner_model_b`

## ğŸ“ Project Structure

```
ChatBot-Solution/
â”œâ”€â”€ ğŸ“‚ configs/                 # YAML configuration files for different models
â”‚   â”œâ”€â”€ gemma_rm.yaml           # Gemma reward model config
â”‚   â”œâ”€â”€ llama_3.yaml            # Llama-3 config
â”‚   â”œâ”€â”€ pair_pref.yaml          # Pair preference model config
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“‚ models/                  # Custom model implementations
â”‚   â”œâ”€â”€ llm_model.py           # LLM-based reward model
â”‚   â”œâ”€â”€ biencoder.py           # Bi-encoder architecture
â”‚   â”œâ”€â”€ sw_transformer.py      # Sliding window transformer
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“‚ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ generate.py            # Text generation script
â”‚   â”œâ”€â”€ vllm_generate.py       # VLLM-based generation for pseudo-labeling
â”‚   â””â”€â”€ awq_quantize.py        # Model quantization
â”œâ”€â”€ ğŸ“‚ deepspeed/               # DeepSpeed configuration
â”‚   â””â”€â”€ zero2.json
â”œâ”€â”€ ğŸ“‚ data/                    # Data directory
â”‚   â”œâ”€â”€ train.csv              # Training data
â”‚   â”œâ”€â”€ test.csv               # Test data
â”‚   â”œâ”€â”€ preds/                 # Predictions output
â”‚   â””â”€â”€ pseudo/                # Pseudo labels
â”œâ”€â”€ ğŸ ce_train.py             # Cross-encoder training
â”œâ”€â”€ ğŸ llm_qlora.py            # LLM QLoRA training
â”œâ”€â”€ ğŸ llm_validate.py         # Model validation/inference
â”œâ”€â”€ ğŸ llm_pseudo_label.py     # Pseudo-label generation
â”œâ”€â”€ ğŸ llm_train_pseudo.py     # Training on pseudo labels
â”œâ”€â”€ ğŸ dpo_train.py            # Direct Preference Optimization
â”œâ”€â”€ ğŸ sft_train.py            # Supervised Fine-Tuning
â”œâ”€â”€ ğŸ siamese_train.py        # Siamese network training
â”œâ”€â”€ ğŸ sequence_train.py       # Sequence model training
â”œâ”€â”€ ğŸ process_data.py         # Data preprocessing
â”œâ”€â”€ ğŸ oof_optimization.py     # Out-of-fold ensemble optimization
â”œâ”€â”€ ğŸ“œ run_stage_1.sh          # Stage 1 training script
â”œâ”€â”€ ğŸ“œ run_stage_2.sh          # Stage 2 training script
â”œâ”€â”€ ğŸ“„ environment.yml         # Conda environment file
â”œâ”€â”€ ğŸ“„ LICENSE                 # MIT License
â””â”€â”€ ğŸ“– README.md               # This file
```

## ğŸ”„ Training Pipeline

![Training Flow](docs/images/training_flow.png)
*Figure 3: Detailed training flow diagram*

### ğŸ“ Stage 1: Initial Training and Pseudo-Label Generation

1. **ğŸ“Š Process training data**:
   ```bash
   python process_data.py
   ```
   This script:
   - âœ… Processes competition data (`train.csv`, `test.csv`)
   - ğŸ“¥ Downloads additional datasets from Hugging Face:
     - [lmsys-33k-deduplicated](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)
     - [orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)
   - ğŸ”€ Creates stratified folds for cross-validation
   - ğŸ’¾ Saves processed data as parquet files

2. **ğŸ¯ Generate paired completions for pseudo-labeling** (optional):
   ```bash
   python scripts/vllm_generate.py
   ```
   Generates paired completions for the [lmsys-1m](https://huggingface.co/datasets/lmsys/lmsys-chat-1m) dataset.

3. **ğŸš€ Run Stage 1 training**:
   ```bash
   bash run_stage_1.sh
   ```
   
   This script trains:
   - ğŸ”— Pair preference models (`pair_pref.yaml`)
   - ğŸŸ¢ Gemma reward models (`gemma_rm.yaml`, `gemma_rm_no_cap.yaml`)
   - ğŸ·ï¸ Generates pseudo labels using trained models
   - ğŸ“ˆ Performs out-of-fold optimization

### ğŸ“ Stage 2: Training on Pseudo Labels

1. **ğŸš€ Run Stage 2 training**:
   ```bash
   bash run_stage_2.sh
   ```
   
   This script:
   - ğŸ“ Trains models on pseudo-labeled data
   - ğŸ® Uses configurations optimized for 8 GPUs
   - ğŸ“Š Generates final predictions

### ğŸ® GPU Configuration

| Stage | GPUs | Effective Batch Size | Config |
|-------|------|---------------------|--------|
| **Stage 1** | 2 GPUs | 8 | `batch_size: 4`, `accum: 2` |
| **Stage 2** | 8 GPUs | 8 | `batch_size: 4`, `accum: 2` (per GPU) |

> ğŸ’¡ **Tip**: To adjust for different GPU counts, modify the `batch_size` and `accum` parameters in the config files to maintain the same effective batch size.

## ğŸ’» Usage

### ğŸ“ Training a Single Model

Train a specific model using a configuration file:

```bash
accelerate launch llm_qlora.py -C configs/gemma_rm.yaml
```

> ğŸ“š Learn more about [Accelerate](https://huggingface.co/docs/accelerate) for distributed training

### âœ… Validation/Inference

Run inference on a dataset:

```bash
# Standard inference
accelerate launch llm_validate.py -C configs/gemma_rm.yaml

# With test-time augmentation
accelerate launch llm_validate.py -C configs/gemma_rm.yaml --tta
```

### ğŸ·ï¸ Pseudo-Label Generation

Generate pseudo labels using a trained model:

```bash
# Standard
accelerate launch llm_pseudo_label.py -C configs/gemma_rm.yaml

# With TTA
accelerate launch llm_pseudo_label.py -C configs/gemma_rm.yaml --tta
```

### ğŸ“Š Custom Dataset Inference

To run inference on your own dataset:

1. Replace `train.parquet` in `llm_validate.py` with your dataset
2. Ensure your dataset has the same structure (prompt, response_a, response_b columns)
3. Run validation:
   ```bash
   accelerate launch llm_validate.py -C configs/<your_config>.yaml
   ```

### ğŸ¯ Ensemble Predictions

For TTA-based ensemble:
1. Run validation with `--tta` flag
2. Run validation without `--tta` flag
3. Ensemble the saved prediction files using `oof_optimization.py`

## âš™ï¸ Model Configurations

Configuration files are located in `configs/` and specify:

- **ğŸ¤– Model**: Base model name from Hugging Face
- **ğŸ“Š Training parameters**: Learning rate, epochs, batch size, gradient accumulation
- **ğŸ”§ LoRA parameters**: Rank, alpha, target modules
- **ğŸ“ Sequence length**: Max length for training and validation

### ğŸ“ Example Configuration

Example configuration (`configs/gemma_rm.yaml`):

```yaml
model_name: sfairXC/FsfairX-Gemma2-RM-v0.1
exp_name: llm_surround_no_lstm

lr: 1.0e-4
epochs: 1
weight_decay: 0.01

lora_r: 64
lora_alpha: 16
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

training:
  batch_size: 4
  accum: 2
  max_length: 1800

validation:
  batch_size: 4
  accum: 1
  max_length: 8192
```

> ğŸ“– See all available configurations in the [`configs/`](configs/) directory

## ğŸ”§ Key Components

### ğŸ“ Training Scripts

| Script | Description | Documentation |
|--------|-------------|--------------|
| **`llm_qlora.py`** | QLoRA fine-tuning for LLM-based reward models | [QLoRA Paper](https://arxiv.org/abs/2305.14314) |
| **`ce_train.py`** | Cross-encoder training (DeBERTa-based) | [DeBERTa Paper](https://arxiv.org/abs/2006.03654) |
| **`dpo_train.py`** | Direct Preference Optimization training | [DPO Paper](https://arxiv.org/abs/2305.18290) |
| **`sft_train.py`** | Supervised Fine-Tuning for language models | - |
| **`siamese_train.py`** | Siamese network training for bi-encoders | - |
| **`sequence_train.py`** | Sequence classification model training | - |
| **`pseudo_ce_train.py`** | Cross-encoder training with pseudo labels | - |

### ğŸ—ï¸ Model Implementations

- **`models/llm_model.py`**: Custom LLM-based reward model wrapper
- **`models/biencoder.py`**: Bi-encoder architecture for separate encoding
- **`models/sw_transformer.py`**: Sliding window transformer for long sequences
- **`models/positional_embedding.py`**: Custom positional embeddings

### ğŸ“Š Data Processing

- **`process_data.py`**: Main data preprocessing pipeline
- **`process_ultrafeedback.py`**: [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset processing
- **`pseudo_label.py`**: Pseudo-label generation utilities

### ğŸ¯ Optimization

- **`oof_optimization.py`**: Out-of-fold ensemble optimization
- **`generate_evaluation.py`**: Evaluation metric generation

## ğŸ“Š Training Techniques

### ğŸ·ï¸ Pseudo-Labeling

![Pseudo-Labeling Process](docs/images/pseudo_labeling.png)
*Figure 4: Pseudo-labeling workflow*

The solution uses iterative pseudo-labeling:
1. ğŸ“ Train initial models on labeled data
2. ğŸ”® Generate predictions on unlabeled data
3. âœ… Use high-confidence predictions as pseudo labels
4. ğŸ”„ Retrain models on combined labeled + pseudo-labeled data

### ğŸ” Test-Time Augmentation (TTA)

TTA improves robustness by:
- ğŸ”€ Swapping response_a and response_b
- ğŸ“Š Averaging predictions from both configurations
- ğŸ¯ Reducing bias toward response ordering

### âš¡ QLoRA Fine-Tuning

Efficient fine-tuning using:
- ğŸ”¢ 4-bit quantization ([BitsAndBytes](https://github.com/TimDettmers/bitsandbytes))
- ğŸ”§ LoRA adapters (rank 64, alpha 16) ([PEFT](https://github.com/huggingface/peft))
- ğŸ’¾ Gradient checkpointing
- ğŸ¨ Mixed precision training (bf16)

> ğŸ“š Learn more: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

## ğŸ“ Notes

- âœ… All models use cross-validation with 4-5 folds
- ğŸ“Š Training uses [Weights & Biases](https://wandb.ai/) (wandb) for logging
- ğŸ’¾ Checkpoints are saved to `/mnt/one/kaggle/lmsys-chatbot-arena/` (modify in scripts)
- â˜ï¸ The solution was trained on a combination of local and cloud GPUs
- ğŸ”§ Modify paths in scripts to match your setup

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ™ Acknowledgments

- ğŸ† This solution achieved **3rd place** in the [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) competition
- ğŸ¤— The implementation leverages multiple open-source models and datasets from the [Hugging Face](https://huggingface.co/) ecosystem
- ğŸ“š Special thanks to:
  - [LMSYS](https://lmsys.org/) for organizing the competition
  - [Hugging Face](https://huggingface.co/) for model hosting and datasets
  - [Unsloth](https://github.com/unslothai/unsloth) for efficient training utilities
  - [VLLM](https://github.com/vllm-project/vllm) for fast inference
  - [Accelerate](https://github.com/huggingface/accelerate) for distributed training

## ğŸ”— Useful Links

- ğŸŒ [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena)
- ğŸ“Š [Competition Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- ğŸ¤— [Hugging Face Models](https://huggingface.co/models)
- ğŸ“š [Transformers Documentation](https://huggingface.co/docs/transformers)
- ğŸš€ [Accelerate Documentation](https://huggingface.co/docs/accelerate)
- ğŸ”§ [PEFT Documentation](https://huggingface.co/docs/peft)
- âš¡ [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- ğŸ¯ [DPO Paper](https://arxiv.org/abs/2305.18290)

---

<div align="center">

**â­ If you find this project useful, please consider giving it a star! â­**

Made with â¤ï¸ for the ML community

</div>
